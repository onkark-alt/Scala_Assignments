package Assignment3

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object ParquetToJsonAggregation extends App {

  println("üöÄ Starting Pipeline 3: Parquet ‚Üí Aggregation ‚Üí JSON")

  val spark = SparkSession.builder()
    .appName("ParquetToJsonAggregation")
    .master("local[*]")
    .getOrCreate()

  // ---- ADD S3 CREDENTIALS ----
  spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", "access-key")
  spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", "secret-key")
  spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.eu-north-1.amazonaws.com")

  // ---------------------------------------------
  // 1Ô∏è‚É£ READ PARQUET FROM S3
  // ---------------------------------------------
  println("üì• Reading Parquet files from S3...")

  val parquetDF = spark.read
    .parquet("s3a://user-s3/sales/parquet/")

  parquetDF.show(10, false)

  // ---------------------------------------------
  // 2Ô∏è‚É£ COMPUTE AGGREGATION
  // ---------------------------------------------
  println("üßÆ Computing totals per product...")

  val aggDF = parquetDF
    .groupBy("product_name")
    .agg(
      sum("quantity").alias("total_quantity"),
      sum("amount").alias("total_revenue")
    )
    .orderBy(desc("total_revenue"))

  aggDF.show(20, false)

  // ---------------------------------------------
  // 3Ô∏è‚É£ WRITE JSON TO S3
  // ---------------------------------------------
  val outputPath = "s3a://user-s3/aggregates/products.json"

  println(s"üíæ Writing JSON output to: $outputPath")

  aggDF.coalesce(1)
    .write
    .mode("overwrite")
    .json(outputPath)

  println("‚úÖ JSON aggregation job completed successfully!")

  spark.stop()
}
